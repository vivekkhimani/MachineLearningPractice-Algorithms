*** NAIVE BAYES CLASSIFIER ***
---
Credits: Analytics Vidhya (https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)
scikitlearn documenation: https://scikit-learn.org/stable/modules/naive_bayes.html
---

1. General Info:
- One of the SIMPLEST CLASSIFICATION MODEL which is based on the Bayes Theorem (Conditional Probability).
- Usually works and preferred for quantitatively large datasets (MANY DATA POINTS or ROWS), with less features/predictors/columns. 
- Assumes that the features/predictors/columns are INDEPENDENTLY related to each other. 
- Example: Apple has features like - Shape, Size, and Color which are independent to each other. (Credits - Analytics Vidhya)
------------------------

2. Math or Logic:
- The entire Math and Logic behind the working of this model can be found on the website on top of the page (with example).
------------------------

3. Pros and Cons:
-> Pros:
	a. Works for single-class and multi-class classification problems with similar logic. 
	b. Works quite fast with large datasets which has minimum or less number of features. 
	c. If the features are completely independent, it almost yields better results than logistic regression or other more complicated classfier. 
	d. Generally performs well with CATEGORICAL INPUT (one in the example above) than the numerical ones. For the numerical ones, a BELL CURVE is assumed (which is a strong
		assumption as it's hard to have one)

-> Cons:
	a. If an unknown CATEGORY is found in TEST DATA which wasn't present in TRAINING DATA, a probability of 0 is assigned which might interfere with the results. To solve this,
		a smoothing technique called "Laplace Estimation". Please note that we cannot take a ZERO PROBABILITY for any category because ZERO CATEGORY would mean that
		it's IMPOSSIBLE for such a data to exist. That's a CONTRADICTION because the data is existing in test set. Also, logically and ethically, if a category cannot
		be found in training set, it doesn't mean that it cannot be found at all in real-world. That's why we need to use SMOOTHING TECHNIQUES!
	
	b. It's almost impossible to find data whose predictors are INDEPENDENT. So assuming the independence among the features is rather a hard assumption. 
----------------------- 

4. Applications of Naive-Bayes Classifier can be found on the website link!

----------------------

5. Building a Model Using Python (scikit learn) and R:
- I will upload the codes for different scikit learn models and examples in this folder later on but the basic information can be found on the webpage link again. 

---------------------

6. Improving a Naive-Bayes classifier:
- As mentioned earlier, Naive-Bayes classifier assumes a Normal Distribution for Numerical Data which is a strong assumption. If the normal distribution is not found, various
	transformation techniques should be use to convert the data into a normal distribution.
- Zero frequency or probability issue (discussed earlier) in the test data can be resolved by various smoothing techniques (LAPLACE's SMOOTHING).
- Removing correlated features as Naive Bayes classifier assumes independence of features.
- Because of its nature and mechanism, there are limited options of parameter tuning in Naive Bayes classifier. So it's often advisable to focus on DATA PRE-PROCESSING and
	FEATURE ENGINEERING to get better results.
- Other classification combination techniques to reduce variance won't work because Naive Bayes classifier has no variance at all. 